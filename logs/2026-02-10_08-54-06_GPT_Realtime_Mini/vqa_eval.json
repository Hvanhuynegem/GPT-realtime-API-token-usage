{
  "summary": {
    "dataset": "data\\VQA-HMUG-data\\prepared.json",
    "responses": "logs\\2026-02-10_08-54-06_GPT_Realtime_Mini\\responses.jsonl",
    "total_response_lines": 400,
    "matched_predictions": 400,
    "missing_in_dataset": 0,
    "overall_accuracy_official_vqa": 0.4685000000000001,
    "per_preprocessor": {
      "Baseline_NoOp": {
        "n": 50,
        "accuracy": 0.45399999999999996
      },
      "Downsampler": {
        "n": 50,
        "accuracy": 0.47400000000000014
      },
      "GazeRoi+GlobalThumb": {
        "n": 50,
        "accuracy": 0.44800000000000006
      },
      "Grayscale": {
        "n": 50,
        "accuracy": 0.442
      },
      "Jpeg_Q85": {
        "n": 50,
        "accuracy": 0.546
      },
      "SalientRoi+GlobalThumb": {
        "n": 50,
        "accuracy": 0.466
      },
      "WebP_Lossy_Q85": {
        "n": 50,
        "accuracy": 0.44400000000000006
      },
      "YoloV12SalientRoi+GlobalThumb": {
        "n": 50,
        "accuracy": 0.47400000000000003
      }
    }
  },
  "details": [
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing on paper",
      "prediction_used_for_eval": "writing on paper",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "butcher block",
      "prediction_used_for_eval": "butcher block",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red-brown",
      "prediction_used_for_eval": "red brown",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1 person",
      "prediction_used_for_eval": "1 person",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo sticks",
      "prediction_used_for_eval": "bamboo sticks",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left hand",
      "prediction_used_for_eval": "left hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "oval",
      "prediction_used_for_eval": "oval",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink blue",
      "prediction_used_for_eval": "purple pink blue",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "six people",
      "prediction_used_for_eval": "6 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "around 3 years",
      "prediction_used_for_eval": "around 3 years",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "ski gear",
      "prediction_used_for_eval": "ski gear",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "maybe",
      "prediction_used_for_eval": "maybe",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3 sheep",
      "prediction_used_for_eval": "3 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, but I can't determine the ethnicity of the batter from the image. Let me know if there's anything else I can help with.",
      "prediction_used_for_eval": "i'm sorry but i can't determine ethnicity of batter from image let me know if there's anything else i can help with",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "underwater enclosure",
      "prediction_used_for_eval": "underwater enclosure",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "wait for car",
      "prediction_used_for_eval": "wait for car",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "washing dishes",
      "prediction_used_for_eval": "washing dishes",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right hand",
      "prediction_used_for_eval": "right hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple blue",
      "prediction_used_for_eval": "purple blue",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6",
      "prediction_used_for_eval": "6",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no trees",
      "prediction_used_for_eval": "no trees",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "smiling",
      "prediction_used_for_eval": "smiling",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "yes apples",
      "prediction_used_for_eval": "yes apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2 children",
      "prediction_used_for_eval": "2 children",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "2 children",
      "prediction_used_for_eval": "2 children",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "ski equipment",
      "prediction_used_for_eval": "ski equipment",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4 sheep",
      "prediction_used_for_eval": "4 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I can't determine ethnicity.",
      "prediction_used_for_eval": "i can't determine ethnicity",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes event",
      "prediction_used_for_eval": "yes event",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "pool enclosure",
      "prediction_used_for_eval": "pool enclosure",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "1 plant",
      "prediction_used_for_eval": "1 plant",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "traffic light",
      "prediction_used_for_eval": "traffic light",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "light blue",
      "prediction_used_for_eval": "light blue",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "cleaning kitchen",
      "prediction_used_for_eval": "cleaning kitchen",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right hand",
      "prediction_used_for_eval": "right hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "dark gray",
      "prediction_used_for_eval": "dark gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple logo",
      "prediction_used_for_eval": "apple logo",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "red black white",
      "prediction_used_for_eval": "red black white",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "8 people",
      "prediction_used_for_eval": "8 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no trees",
      "prediction_used_for_eval": "no trees",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "old car",
      "prediction_used_for_eval": "old car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "yes, apples",
      "prediction_used_for_eval": "yes apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis poles",
      "prediction_used_for_eval": "skis poles",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "not clear",
      "prediction_used_for_eval": "not clear",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3 sheep",
      "prediction_used_for_eval": "3 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, I can't determine someone's ethnicity from an image.",
      "prediction_used_for_eval": "i'm sorry i can't determine someone's ethnicity from image",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "underwater enclosure",
      "prediction_used_for_eval": "underwater enclosure",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "not safe",
      "prediction_used_for_eval": "not safe",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing on paper",
      "prediction_used_for_eval": "writing on paper",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "2 people",
      "prediction_used_for_eval": "2 people",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left hand",
      "prediction_used_for_eval": "left hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple logo",
      "prediction_used_for_eval": "apple logo",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink blue",
      "prediction_used_for_eval": "purple pink blue",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing practice",
      "prediction_used_for_eval": "dancing practice",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "around 3",
      "prediction_used_for_eval": "around 3",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis poles bindings",
      "prediction_used_for_eval": "skis poles bindings",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3 sheep",
      "prediction_used_for_eval": "3 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, I can't determine ethnicity from the image.",
      "prediction_used_for_eval": "i'm sorry i can't determine ethnicity from image",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "underwater enclosure",
      "prediction_used_for_eval": "underwater enclosure",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "its unsafe",
      "prediction_used_for_eval": "its unsafe",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing list",
      "prediction_used_for_eval": "writing list",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "cream",
      "prediction_used_for_eval": "cream",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left hand",
      "prediction_used_for_eval": "left hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple blue pink",
      "prediction_used_for_eval": "purple blue pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "5 people",
      "prediction_used_for_eval": "5 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no trees",
      "prediction_used_for_eval": "no trees",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing practice",
      "prediction_used_for_eval": "dancing practice",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "overripe",
      "prediction_used_for_eval": "overripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "under 3",
      "prediction_used_for_eval": "under 3",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "zero",
      "prediction_used_for_eval": "zero",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "partly cloudy",
      "prediction_used_for_eval": "partly cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "ski equipment",
      "prediction_used_for_eval": "ski equipment",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, but I can't determine someone's ethnicity from an image. Let me know if there's anything else I can help with.",
      "prediction_used_for_eval": "i'm sorry but i can't determine someone's ethnicity from image let me know if there's anything else i can help with",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "caution required",
      "prediction_used_for_eval": "caution required",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing on paper",
      "prediction_used_for_eval": "writing on paper",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1 person",
      "prediction_used_for_eval": "1 person",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "off-white",
      "prediction_used_for_eval": "off white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right hand",
      "prediction_used_for_eval": "right hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "oval",
      "prediction_used_for_eval": "oval",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple white pink",
      "prediction_used_for_eval": "purple white pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7 people",
      "prediction_used_for_eval": "7 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no trees",
      "prediction_used_for_eval": "no trees",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "spotted black",
      "prediction_used_for_eval": "spotted black",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "underage three",
      "prediction_used_for_eval": "underage 3",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis poles backpacks",
      "prediction_used_for_eval": "skis poles backpacks",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "not clear",
      "prediction_used_for_eval": "not clear",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes, left table",
      "prediction_used_for_eval": "yes, left table",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3 sheep",
      "prediction_used_for_eval": "3 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, I can't determine the batter's ethnicity from the image. Let me know if there's anything else I can help with.",
      "prediction_used_for_eval": "i'm sorry i can't determine batter's ethnicity from image let me know if there's anything else i can help with",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal frame",
      "prediction_used_for_eval": "metal frame",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no person visible",
      "prediction_used_for_eval": "no person visible",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium enclosure",
      "prediction_used_for_eval": "aquarium enclosure",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "not safe",
      "prediction_used_for_eval": "not safe",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing list",
      "prediction_used_for_eval": "writing list",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1 person",
      "prediction_used_for_eval": "1 person",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "beige",
      "prediction_used_for_eval": "beige",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left hand",
      "prediction_used_for_eval": "left hand",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink blue",
      "prediction_used_for_eval": "purple pink blue",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "six people",
      "prediction_used_for_eval": "6 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no trees",
      "prediction_used_for_eval": "no trees",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing practice",
      "prediction_used_for_eval": "dancing practice",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3 children",
      "prediction_used_for_eval": "3 children",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "ski gear",
      "prediction_used_for_eval": "ski gear",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "not clear",
      "prediction_used_for_eval": "not clear",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, but I can't determine ethnicity from appearance alone. Let me know if you'd like help with anything else.",
      "prediction_used_for_eval": "i'm sorry but i can't determine ethnicity from appearance alone let me know if you'd like help with anything else",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal wood fabric",
      "prediction_used_for_eval": "metal wood fabric",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no person visible",
      "prediction_used_for_eval": "no person visible",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes event",
      "prediction_used_for_eval": "yes event",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "not safe",
      "prediction_used_for_eval": "not safe",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing on clipboard",
      "prediction_used_for_eval": "writing on clipboard",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "2 people",
      "prediction_used_for_eval": "2 people",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "woven bamboo",
      "prediction_used_for_eval": "woven bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple logo",
      "prediction_used_for_eval": "apple logo",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple blue pink",
      "prediction_used_for_eval": "purple blue pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6 people",
      "prediction_used_for_eval": "6 people",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "crowd",
      "prediction_used_for_eval": "crowd",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "around 3",
      "prediction_used_for_eval": "around 3",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2 children",
      "prediction_used_for_eval": "2 children",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "one",
      "prediction_used_for_eval": "one",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis poles",
      "prediction_used_for_eval": "skis poles",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unclear appearance",
      "prediction_used_for_eval": "unclear appearance",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3 sheep",
      "prediction_used_for_eval": "3 sheep",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, I can't determine the ethnicity of the batter from the image. Let me know if you need any other details.",
      "prediction_used_for_eval": "i'm sorry i can't determine ethnicity of batter from image let me know if you need any other details",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal and wood",
      "prediction_used_for_eval": "metal and wood",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "cosplay event",
      "prediction_used_for_eval": "cosplay event",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "zoo",
      "prediction_used_for_eval": "zoo",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2 plants",
      "prediction_used_for_eval": "2 plants",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "its risky",
      "prediction_used_for_eval": "its risky",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    }
  ]
}