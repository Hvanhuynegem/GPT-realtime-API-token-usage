{
  "summary": {
    "dataset": "data\\VQA-HMUG-data\\prepared.json",
    "responses": "logs\\2026-02-11_08-10-27_GPT_Realtime_Mini\\responses.jsonl",
    "total_response_lines": 700,
    "matched_predictions": 700,
    "missing_in_dataset": 0,
    "overall_accuracy_official_vqa": 0.6314285714285715,
    "per_preprocessor": {
      "Baseline_NoOp": {
        "n": 100,
        "accuracy": 0.73
      },
      "Downsampler": {
        "n": 100,
        "accuracy": 0.6289999999999999
      },
      "GazeRoi+GlobalThumb": {
        "n": 100,
        "accuracy": 0.504
      },
      "Grayscale": {
        "n": 100,
        "accuracy": 0.6619999999999999
      },
      "Jpeg_Q85": {
        "n": 100,
        "accuracy": 0.709
      },
      "SalientRoi+GlobalThumb": {
        "n": 100,
        "accuracy": 0.4749999999999999
      },
      "WebP_Lossy_Q85": {
        "n": 100,
        "accuracy": 0.711
      }
    }
  },
  "details": [
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple yellow",
      "prediction_used_for_eval": "purple yellow",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "autumn",
      "prediction_used_for_eval": "autumn",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "storefront",
      "prediction_used_for_eval": "storefront",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "near keyboard",
      "prediction_used_for_eval": "near keyboard",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "soccer",
      "prediction_used_for_eval": "soccer",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "ponytail",
      "prediction_used_for_eval": "ponytail",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "cooking",
      "prediction_used_for_eval": "cooking",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink",
      "prediction_used_for_eval": "purple pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.9
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "neutral",
      "prediction_used_for_eval": "neutral",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "35",
      "prediction_used_for_eval": "35",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "13",
      "prediction_used_for_eval": "13",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "catching",
      "prediction_used_for_eval": "catching",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "somewhat",
      "prediction_used_for_eval": "somewhat",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "apartment",
      "prediction_used_for_eval": "apartment",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "even",
      "prediction_used_for_eval": "even",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sitting",
      "prediction_used_for_eval": "sitting",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "wall street",
      "prediction_used_for_eval": "wall street",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "informal",
      "prediction_used_for_eval": "informal",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "unclear",
      "prediction_used_for_eval": "unclear",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "none",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "tied",
      "prediction_used_for_eval": "tied",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unclear",
      "prediction_used_for_eval": "unclear",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "teal",
      "prediction_used_for_eval": "teal",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "unpacking",
      "prediction_used_for_eval": "unpacking",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "black white",
      "prediction_used_for_eval": "black white",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "maybe",
      "prediction_used_for_eval": "maybe",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "none",
      "prediction_used_for_eval": "none",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "underwater",
      "prediction_used_for_eval": "underwater",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "catching",
      "prediction_used_for_eval": "catching",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "drlinger",
      "prediction_used_for_eval": "drlinger",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "storefront",
      "prediction_used_for_eval": "storefront",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top",
      "prediction_used_for_eval": "top",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "formal",
      "prediction_used_for_eval": "formal",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "soccer",
      "prediction_used_for_eval": "soccer",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "ponytail",
      "prediction_used_for_eval": "ponytail",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "spreading",
      "prediction_used_for_eval": "spreading",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "cobblestone",
      "prediction_used_for_eval": "cobblestone",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "pink",
      "prediction_used_for_eval": "pink",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "flower",
      "prediction_used_for_eval": "flower",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink",
      "prediction_used_for_eval": "purple pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "checking phones",
      "prediction_used_for_eval": "checking phones",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry, unable",
      "prediction_used_for_eval": "sorry unable",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "catching",
      "prediction_used_for_eval": "catching",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "daimler",
      "prediction_used_for_eval": "daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "storefront",
      "prediction_used_for_eval": "storefront",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "concerned",
      "prediction_used_for_eval": "concerned",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "even",
      "prediction_used_for_eval": "even",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "semi-formal",
      "prediction_used_for_eval": "semi formal",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "soccer",
      "prediction_used_for_eval": "soccer",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "pulled-back",
      "prediction_used_for_eval": "pulled back",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "looking",
      "prediction_used_for_eval": "looking",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "zero",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "off-white",
      "prediction_used_for_eval": "off white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple yellow pink",
      "prediction_used_for_eval": "purple yellow pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "8",
      "prediction_used_for_eval": "8",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very",
      "prediction_used_for_eval": "very",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "likely",
      "prediction_used_for_eval": "likely",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "water",
      "prediction_used_for_eval": "water",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "wait",
      "prediction_used_for_eval": "wait",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "batting",
      "prediction_used_for_eval": "batting",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "apartment",
      "prediction_used_for_eval": "apartment",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "soccer",
      "prediction_used_for_eval": "soccer",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "ponytail",
      "prediction_used_for_eval": "ponytail",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "spreading",
      "prediction_used_for_eval": "spreading",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "painting",
      "prediction_used_for_eval": "painting",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "unclear",
      "prediction_used_for_eval": "unclear",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple white",
      "prediction_used_for_eval": "purple white",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "lights",
      "prediction_used_for_eval": "lights",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "truck",
      "prediction_used_for_eval": "truck",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "9",
      "prediction_used_for_eval": "9",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "underwater",
      "prediction_used_for_eval": "underwater",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "wait",
      "prediction_used_for_eval": "wait",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "running",
      "prediction_used_for_eval": "running",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "daimler",
      "prediction_used_for_eval": "daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "store",
      "prediction_used_for_eval": "store",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "morning",
      "prediction_used_for_eval": "morning",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "desk",
      "prediction_used_for_eval": "desk",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "tennis",
      "prediction_used_for_eval": "tennis",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "braided",
      "prediction_used_for_eval": "braided",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "standing",
      "prediction_used_for_eval": "standing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple white",
      "prediction_used_for_eval": "purple white",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "woven",
      "prediction_used_for_eval": "woven",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "lights",
      "prediction_used_for_eval": "lights",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "yellow",
      "prediction_used_for_eval": "yellow",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "backpacks",
      "prediction_used_for_eval": "backpacks",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "rectangular",
      "prediction_used_for_eval": "rectangular",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "19",
      "prediction_used_for_eval": "19",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "water",
      "prediction_used_for_eval": "water",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "Daimler",
      "prediction_used_for_eval": "Daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "store",
      "prediction_used_for_eval": "store",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "morning",
      "prediction_used_for_eval": "morning",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "even",
      "prediction_used_for_eval": "even",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "looking",
      "prediction_used_for_eval": "looking",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "clock",
      "prediction_used_for_eval": "clock",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "unspecified",
      "prediction_used_for_eval": "unspecified",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "desk",
      "prediction_used_for_eval": "desk",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12062002",
      "question": "What kind of ball is present?",
      "prediction_raw": "fluffy",
      "prediction_used_for_eval": "fluffy",
      "human_answers_used_for_eval": [
        "volleyball",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer",
        "soccer ball",
        "soccer",
        "soccer",
        "fur ball"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12085017",
      "question": "Is the man wearing glasses?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12230002",
      "question": "What color is the car on the right?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "dark blue",
        "dark blue",
        "blue",
        "dark blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12238002",
      "question": "What is the man doing?",
      "prediction_raw": "riding",
      "prediction_used_for_eval": "riding",
      "human_answers_used_for_eval": [
        "riding motorcycle",
        "riding",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "sitting",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle",
        "riding motorcycle"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12448000",
      "question": "Is the baby bald?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12455001",
      "question": "What is the man doing on the end of the dock?",
      "prediction_raw": "fishing",
      "prediction_used_for_eval": "fishing",
      "human_answers_used_for_eval": [
        "sitting",
        "sitting",
        "fishing",
        "fishing",
        "sitting",
        "sitting",
        "fishing",
        "sitting",
        "sitting",
        "fishing"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12543002",
      "question": "How is the girl's hair styled?",
      "prediction_raw": "tied",
      "prediction_used_for_eval": "tied",
      "human_answers_used_for_eval": [
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail",
        "simple",
        "ponytail",
        "ponytail",
        "ponytail",
        "ponytail"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12700001",
      "question": "Are these crocheted doilies?",
      "prediction_raw": "crocheted",
      "prediction_used_for_eval": "crocheted",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12733000",
      "question": "What season is it?",
      "prediction_raw": "winter",
      "prediction_used_for_eval": "winter",
      "human_answers_used_for_eval": [
        "winter",
        "winter",
        "winter",
        "winter",
        "winter",
        "christmas",
        "christmas",
        "winter",
        "winter",
        "winter"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12764004",
      "question": "Does the dog looked sad?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "13031001",
      "question": "What shape is the water splash making in the air?",
      "prediction_raw": "arc",
      "prediction_used_for_eval": "arc",
      "human_answers_used_for_eval": [
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "circle",
        "arc",
        "circle",
        "circle",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "13414003",
      "question": "Are any cars parked at the meters?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "13490000",
      "question": "Are they eating this plain?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "13991003",
      "question": "Is the stove gas or electric?",
      "prediction_raw": "gas",
      "prediction_used_for_eval": "gas",
      "human_answers_used_for_eval": [
        "gas",
        "gas",
        "gas",
        "electric",
        "gas",
        "gas",
        "gas",
        "gas",
        "has",
        "electric"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14056001",
      "question": "How many shark fins are poking out from the water?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "0",
        "0",
        "0",
        "0",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14088004",
      "question": "Is he happy?",
      "prediction_raw": "unclear",
      "prediction_used_for_eval": "unclear",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14151003",
      "question": "Is the person snowboarding?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14226000",
      "question": "What color is the boys jacket?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "blue"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14226002",
      "question": "What color is the man suit?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "green",
        "teal",
        "green",
        "black",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14226005",
      "question": "What color is the shirt of the man closest to the camera?",
      "prediction_raw": "green",
      "prediction_used_for_eval": "green",
      "human_answers_used_for_eval": [
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green",
        "green"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14282001",
      "question": "How many sinks?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14321002",
      "question": "Is that cobblestone?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14353009",
      "question": "Is this pizza cooked?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14392001",
      "question": "Is the guy point the right way to turn?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "14439001",
      "question": "Are they at a park?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    }
  ]
}