{
  "summary": {
    "dataset": "data\\VQA-HMUG-data\\prepared.json",
    "responses": "logs\\2026-02-12_10-39-28_GPT_Realtime_Mini\\responses.jsonl",
    "total_response_lines": 600,
    "matched_predictions": 600,
    "missing_in_dataset": 0,
    "overall_accuracy_official_vqa": 0.6501666666666667,
    "per_preprocessor": {
      "Baseline_NoOp": {
        "n": 75,
        "accuracy": 0.7
      },
      "Downsampler": {
        "n": 75,
        "accuracy": 0.64
      },
      "GazeRoi+GlobalThumb": {
        "n": 75,
        "accuracy": 0.66
      },
      "Grayscale": {
        "n": 75,
        "accuracy": 0.6733333333333333
      },
      "Jpeg_Q85": {
        "n": 75,
        "accuracy": 0.7266666666666667
      },
      "SalientRoi+GlobalThumb": {
        "n": 75,
        "accuracy": 0.4800000000000001
      },
      "WebP_Lossy_Q85": {
        "n": 75,
        "accuracy": 0.7226666666666666
      },
      "YoloV12SalientRoi+GlobalThumb": {
        "n": 75,
        "accuracy": 0.5986666666666668
      }
    }
  },
  "details": [
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "star",
      "prediction_used_for_eval": "star",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple blue",
      "prediction_used_for_eval": "purple blue",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite festival",
      "prediction_used_for_eval": "kite festival",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "water",
      "prediction_used_for_eval": "water",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "apartment",
      "prediction_used_for_eval": "apartment",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "even",
      "prediction_used_for_eval": "even",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "semi-formal",
      "prediction_used_for_eval": "semi formal",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Baseline_NoOp",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "cooking",
      "prediction_used_for_eval": "cooking",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple pink",
      "prediction_used_for_eval": "purple pink",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6",
      "prediction_used_for_eval": "6",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "posing",
      "prediction_used_for_eval": "posing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "unsure",
      "prediction_used_for_eval": "unsure",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "65",
      "prediction_used_for_eval": "65",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "47",
      "prediction_used_for_eval": "47",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "catching",
      "prediction_used_for_eval": "catching",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "apartment",
      "prediction_used_for_eval": "apartment",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "neutral",
      "prediction_used_for_eval": "neutral",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "relaxing",
      "prediction_used_for_eval": "relaxing",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "barclay",
      "prediction_used_for_eval": "barclay",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Downsampler",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "standing",
      "prediction_used_for_eval": "standing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "red white",
      "prediction_used_for_eval": "red white",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "7",
      "prediction_used_for_eval": "7",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "maybe",
      "prediction_used_for_eval": "maybe",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "batting",
      "prediction_used_for_eval": "batting",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "cluttered",
      "prediction_used_for_eval": "cluttered",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "apartment",
      "prediction_used_for_eval": "apartment",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top center",
      "prediction_used_for_eval": "top center",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "formal",
      "prediction_used_for_eval": "formal",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Grayscale",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple orange",
      "prediction_used_for_eval": "purple orange",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6",
      "prediction_used_for_eval": "6",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "blackened",
      "prediction_used_for_eval": "blackened",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "pool",
      "prediction_used_for_eval": "pool",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "daimler",
      "prediction_used_for_eval": "daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "deserted building",
      "prediction_used_for_eval": "deserted building",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "evening",
      "prediction_used_for_eval": "evening",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "even",
      "prediction_used_for_eval": "even",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "desk",
      "prediction_used_for_eval": "desk",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "Jpeg_Q85",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple yellow",
      "prediction_used_for_eval": "purple yellow",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6",
      "prediction_used_for_eval": "6",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "maybe",
      "prediction_used_for_eval": "maybe",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis backpacks",
      "prediction_used_for_eval": "skis backpacks",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "unknown",
      "prediction_used_for_eval": "unknown",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "aquarium",
      "prediction_used_for_eval": "aquarium",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "daimler",
      "prediction_used_for_eval": "daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "residential",
      "prediction_used_for_eval": "residential",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "night",
      "prediction_used_for_eval": "night",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "WebP_Lossy_Q85",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "apple",
      "prediction_used_for_eval": "apple",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple orange",
      "prediction_used_for_eval": "purple orange",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "6",
      "prediction_used_for_eval": "6",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "ripe",
      "prediction_used_for_eval": "ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "car",
      "prediction_used_for_eval": "car",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "backpacks",
      "prediction_used_for_eval": "backpacks",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "metal",
      "prediction_used_for_eval": "metal",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "water",
      "prediction_used_for_eval": "water",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "somewhat",
      "prediction_used_for_eval": "somewhat",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "brownstone",
      "prediction_used_for_eval": "brownstone",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "morning",
      "prediction_used_for_eval": "morning",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "skateboarding",
      "prediction_used_for_eval": "skateboarding",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "stand",
      "prediction_used_for_eval": "stand",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "GazeRoi+GlobalThumb",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "moderate",
      "prediction_used_for_eval": "moderate",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "standing",
      "prediction_used_for_eval": "standing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "left",
      "prediction_used_for_eval": "left",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple yellow",
      "prediction_used_for_eval": "purple yellow",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wicker",
      "prediction_used_for_eval": "wicker",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "lights",
      "prediction_used_for_eval": "lights",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "painting",
      "prediction_used_for_eval": "painting",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "ripe",
      "prediction_used_for_eval": "ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "jeep",
      "prediction_used_for_eval": "jeep",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "smiling",
      "prediction_used_for_eval": "smiling",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "backpacks",
      "prediction_used_for_eval": "backpacks",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "shut",
      "prediction_used_for_eval": "shut",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "snowboarding",
      "prediction_used_for_eval": "snowboarding",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "rectangular",
      "prediction_used_for_eval": "rectangular",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6000000000000001
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "water",
      "prediction_used_for_eval": "water",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "1",
      "prediction_used_for_eval": "1",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "spring",
      "prediction_used_for_eval": "spring",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "white",
      "prediction_used_for_eval": "white",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "benz",
      "prediction_used_for_eval": "benz",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "brick",
      "prediction_used_for_eval": "brick",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "morning",
      "prediction_used_for_eval": "morning",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "smiling",
      "prediction_used_for_eval": "smiling",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "speaking",
      "prediction_used_for_eval": "speaking",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "clock",
      "prediction_used_for_eval": "clock",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "unclear",
      "prediction_used_for_eval": "unclear",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top-right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "SalientRoi+GlobalThumb",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "338002",
      "question": "What is the man doing?",
      "prediction_raw": "writing",
      "prediction_used_for_eval": "writing",
      "human_answers_used_for_eval": [
        "writing",
        "writing",
        "writing",
        "writing",
        "standing",
        "writing list",
        "writing",
        "writing",
        "eating",
        "holding bag"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "387002",
      "question": "Does the asus laptop have a white USB cable attached?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "486003",
      "question": "What material is the counter made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "699002",
      "question": "What color is the streak?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "gray",
        "orange",
        "red",
        "red",
        "red",
        "red",
        "red",
        "gray"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1000004",
      "question": "How many people have a camera?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "0",
        "1",
        "0",
        "1",
        "0",
        "0",
        "1",
        "0",
        "0",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1205000",
      "question": "What is the material of the walls?",
      "prediction_raw": "bamboo",
      "prediction_used_for_eval": "bamboo",
      "human_answers_used_for_eval": [
        "bamboo",
        "wicker",
        "grass skirt",
        "straw",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo",
        "bamboo"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1342027",
      "question": "What color paper is he holding?",
      "prediction_raw": "yellow",
      "prediction_used_for_eval": "yellow",
      "human_answers_used_for_eval": [
        "cream",
        "white",
        "white",
        "white",
        "white",
        "beige",
        "black and white",
        "white",
        "white",
        "black and white"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1342078",
      "question": "Which hand is the person using to hold the book?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "right",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1398001",
      "question": "What color is the sweater that the person is wearing?",
      "prediction_raw": "gray",
      "prediction_used_for_eval": "gray",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1503002",
      "question": "Is the mouse wireless?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1503003",
      "question": "What is the shape of the silver logo on the base of the center monitor?",
      "prediction_raw": "circle",
      "prediction_used_for_eval": "circle",
      "human_answers_used_for_eval": [
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple",
        "apple"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "1525025",
      "question": "Was the person taking this picture moving?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "2006001",
      "question": "What colors are on the bus?",
      "prediction_raw": "purple orange",
      "prediction_used_for_eval": "purple orange",
      "human_answers_used_for_eval": [
        "purple",
        "purple",
        "purple",
        "purple",
        "purple blue red orange",
        "purple and red",
        "purple white yellow",
        "purple",
        "purple",
        "purple red orange blue white and black"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "2295000",
      "question": "What kind of material is this basket made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "wood",
        "not possible",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "wood",
        "no basket",
        "wood"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3255000",
      "question": "What number of people are standing in front of the slope?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "8",
        "6",
        "6",
        "7",
        "6",
        "5",
        "5",
        "5",
        "6",
        "6"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.9
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3378000",
      "question": "Are those trees at the back?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3693000",
      "question": "What activity are they doing together?",
      "prediction_raw": "dancing",
      "prediction_used_for_eval": "dancing",
      "human_answers_used_for_eval": [
        "dancing",
        "dancing",
        "dancing",
        "dance",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing",
        "dancing"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3703001",
      "question": "How ripe are those bananas?",
      "prediction_raw": "very ripe",
      "prediction_used_for_eval": "very ripe",
      "human_answers_used_for_eval": [
        "very",
        "over ripe",
        "overripe",
        "very ripe",
        "very",
        "very",
        "very",
        "very ripe",
        "very",
        "over ripe"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3711010",
      "question": "What type of vehicle is there?",
      "prediction_raw": "sedan",
      "prediction_used_for_eval": "sedan",
      "human_answers_used_for_eval": [
        "car",
        "car",
        "car",
        "car",
        "sedan",
        "pickup",
        "car",
        "buick",
        "car",
        "car"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3716000",
      "question": "Is the woman happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3742003",
      "question": "Is anything reflected in the mirror?",
      "prediction_raw": "apples",
      "prediction_used_for_eval": "apples",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "apples",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939000",
      "question": "Are the kids under 3 years old?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939001",
      "question": "How many children are holding bears?",
      "prediction_raw": "3",
      "prediction_used_for_eval": "3",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3939002",
      "question": "How many people are not wearing pants?",
      "prediction_raw": "0",
      "prediction_used_for_eval": "0",
      "human_answers_used_for_eval": [
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3947006",
      "question": "Is the sky cloudy or overcast?",
      "prediction_raw": "cloudy",
      "prediction_used_for_eval": "cloudy",
      "human_answers_used_for_eval": [
        "cloudy",
        "yes",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "cloudy",
        "partly cloudy"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "3983001",
      "question": "What type of gear is laying on the ground?",
      "prediction_raw": "skis",
      "prediction_used_for_eval": "skis",
      "human_answers_used_for_eval": [
        "ski",
        "skiing",
        "ski equipment",
        "ski",
        "ski",
        "ski",
        "ski",
        "skis",
        "ski",
        "ski gear"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "4265001",
      "question": "Are the windows open or shut?",
      "prediction_raw": "open",
      "prediction_used_for_eval": "open",
      "human_answers_used_for_eval": [
        "open",
        "shut",
        "shut",
        "open",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut",
        "shut"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "4554002",
      "question": "Are they skiing or snowboarding?",
      "prediction_raw": "skiing",
      "prediction_used_for_eval": "skiing",
      "human_answers_used_for_eval": [
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding",
        "snowboarding"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5060000",
      "question": "Is there a painting or a mirror in the frame?",
      "prediction_raw": "mirror",
      "prediction_used_for_eval": "mirror",
      "human_answers_used_for_eval": [
        "mirror frame",
        "no",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror",
        "mirror"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5123004",
      "question": "Is this building a business or residence?",
      "prediction_raw": "residence",
      "prediction_used_for_eval": "residence",
      "human_answers_used_for_eval": [
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence",
        "residence"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5385001",
      "question": "What is the number of the batter?",
      "prediction_raw": "15",
      "prediction_used_for_eval": "15",
      "human_answers_used_for_eval": [
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15",
        "15"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5388002",
      "question": "Which sport is this?",
      "prediction_raw": "baseball",
      "prediction_used_for_eval": "baseball",
      "human_answers_used_for_eval": [
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball",
        "baseball"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5486002",
      "question": "What shape is the table?",
      "prediction_raw": "round",
      "prediction_used_for_eval": "round",
      "human_answers_used_for_eval": [
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "round",
        "round",
        "round",
        "oval",
        "circular"
      ],
      "used_normalization": true,
      "matches_m_total": 6,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5502000",
      "question": "What kind of festival is happening here?",
      "prediction_raw": "kite",
      "prediction_used_for_eval": "kite",
      "human_answers_used_for_eval": [
        "kite",
        "kite",
        "kite",
        "children's",
        "kite and bouncy playgrounds",
        "kite",
        "kite",
        "kite",
        "kite",
        "kite festival"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5652001",
      "question": "Did the boy just get out of the bath?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "yes",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5694002",
      "question": "What animal is this?",
      "prediction_raw": "cockatoo",
      "prediction_used_for_eval": "cockatoo",
      "human_answers_used_for_eval": [
        "parrot",
        "parrot",
        "bird",
        "parrot",
        "cockatoo",
        "bird",
        "parrot",
        "cockatoo",
        "parrot",
        "parrot"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5816002",
      "question": "Is there water on both end tables?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "5992000",
      "question": "How many sheep are there?",
      "prediction_raw": "5",
      "prediction_used_for_eval": "5",
      "human_answers_used_for_eval": [
        "5",
        "5",
        "7",
        "4",
        "5",
        "4",
        "5",
        "5",
        "many",
        "4"
      ],
      "used_normalization": true,
      "matches_m_total": 5,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6040004",
      "question": "What number is on the front of the train?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2",
        "2"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6471007",
      "question": "What ethnicity is the batter?",
      "prediction_raw": "I'm sorry, I can't determine that.",
      "prediction_used_for_eval": "i'm sorry i can't determine that",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "black",
        "black",
        "african",
        "latino",
        "latino",
        "black",
        "african american"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6580000",
      "question": "What is the bed made of?",
      "prediction_raw": "wood",
      "prediction_used_for_eval": "wood",
      "human_answers_used_for_eval": [
        "cloth",
        "bedding",
        "metal",
        "metal",
        "wood",
        "metal",
        "iron",
        "metal",
        "iron",
        "metal"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6580003",
      "question": "Is there a person on the bed?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6771003",
      "question": "Is this at an event?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "6871002",
      "question": "Where is this bear?",
      "prediction_raw": "pool",
      "prediction_used_for_eval": "pool",
      "human_answers_used_for_eval": [
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "underwater",
        "in water",
        "pool",
        "zoo exhibit water tank",
        "in water"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7072001",
      "question": "Was this photo taken at the park?",
      "prediction_raw": "beach",
      "prediction_used_for_eval": "beach",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7570002",
      "question": "How many plants are there?",
      "prediction_raw": "2",
      "prediction_used_for_eval": "2",
      "human_answers_used_for_eval": [
        "2",
        "2",
        "1",
        "2",
        "2",
        "1",
        "1",
        "1",
        "1",
        "1"
      ],
      "used_normalization": true,
      "matches_m_total": 4,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7726000",
      "question": "Can you cross the street here?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "maybe",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938001",
      "question": "What season is it?",
      "prediction_raw": "fall",
      "prediction_used_for_eval": "fall",
      "human_answers_used_for_eval": [
        "spring",
        "spring",
        "spring",
        "spring",
        "summer",
        "spring",
        "spring",
        "fall",
        "summer",
        "spring"
      ],
      "used_normalization": true,
      "matches_m_total": 1,
      "accuracy_official_vqa": 0.3
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938006",
      "question": "In reference to the man wearing a suit, what color is his shirt?",
      "prediction_raw": "blue",
      "prediction_used_for_eval": "blue",
      "human_answers_used_for_eval": [
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue",
        "blue"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "7938007",
      "question": "Is there a man with his hands in his pocket?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "street",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8065000",
      "question": "What is the guy on the right doing?",
      "prediction_raw": "swinging",
      "prediction_used_for_eval": "swinging",
      "human_answers_used_for_eval": [
        "catching ball",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching",
        "catching"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8065001",
      "question": "Is the batter wearing knee pads?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8204002",
      "question": "Who is the manufacturer of the first car?",
      "prediction_raw": "daimler",
      "prediction_used_for_eval": "daimler",
      "human_answers_used_for_eval": [
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota",
        "toyota"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8401010",
      "question": "How many bears are in the picture?",
      "prediction_raw": "4",
      "prediction_used_for_eval": "4",
      "human_answers_used_for_eval": [
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3",
        "3"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8583000",
      "question": "Is this desk organized well?",
      "prediction_raw": "moderately",
      "prediction_used_for_eval": "moderately",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8771005",
      "question": "Is it winter?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8803002",
      "question": "Does he look like he's dressed for a play?",
      "prediction_raw": "historical",
      "prediction_used_for_eval": "historical",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "8899001",
      "question": "What building is this?",
      "prediction_raw": "store",
      "prediction_used_for_eval": "store",
      "human_answers_used_for_eval": [
        "apartment",
        "brick",
        "mixed use",
        "apartment",
        "hovel",
        "multi use",
        "apartment",
        "apartment",
        "apartment",
        "apartment"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9156003",
      "question": "Where did the sandwich come from?",
      "prediction_raw": "subway",
      "prediction_used_for_eval": "subway",
      "human_answers_used_for_eval": [
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway",
        "subway"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9236007",
      "question": "Is the floor the same material as the cabinets?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9236008",
      "question": "What time of day is it?",
      "prediction_raw": "sorry",
      "prediction_used_for_eval": "sorry",
      "human_answers_used_for_eval": [
        "night",
        "night",
        "evening",
        "nighttime",
        "night time",
        "midnight",
        "night",
        "night time",
        "night",
        "night"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9729001",
      "question": "Does the guy look happy?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no",
        "yes",
        "no",
        "no",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9729002",
      "question": "Which hand could be taking a \"selfie\"?",
      "prediction_raw": "right",
      "prediction_used_for_eval": "right",
      "human_answers_used_for_eval": [
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left",
        "left"
      ],
      "used_normalization": false,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9769001",
      "question": "What color is the truck?",
      "prediction_raw": "red",
      "prediction_used_for_eval": "red",
      "human_answers_used_for_eval": [
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red",
        "red"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9800009",
      "question": "Is there a service man in the picture?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "9800012",
      "question": "What color is the man's tie?",
      "prediction_raw": "black",
      "prediction_used_for_eval": "black",
      "human_answers_used_for_eval": [
        "black",
        "black",
        "black",
        "blue",
        "navy",
        "black",
        "black",
        "black",
        "black",
        "black"
      ],
      "used_normalization": true,
      "matches_m_total": 8,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "10023000",
      "question": "What is he doing?",
      "prediction_raw": "jumping",
      "prediction_used_for_eval": "jumping",
      "human_answers_used_for_eval": [
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skateboarding",
        "skating"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "10216004",
      "question": "Is the woman serving for an odd point or an even point?",
      "prediction_raw": "odd",
      "prediction_used_for_eval": "odd",
      "human_answers_used_for_eval": [
        "unknown",
        "even",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "odd",
        "even",
        "odd"
      ],
      "used_normalization": true,
      "matches_m_total": 7,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "10534005",
      "question": "What is the woman doing?",
      "prediction_raw": "sleeping",
      "prediction_used_for_eval": "sleeping",
      "human_answers_used_for_eval": [
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping",
        "resting",
        "sleeping",
        "sleeping",
        "sleeping",
        "sleeping"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "10793001",
      "question": "IS there anything weird on the train?",
      "prediction_raw": "graffiti",
      "prediction_used_for_eval": "graffiti",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "yes",
        "graffiti",
        "yes",
        "yes",
        "no",
        "yes",
        "graffiti",
        "graffiti"
      ],
      "used_normalization": true,
      "matches_m_total": 3,
      "accuracy_official_vqa": 0.8999999999999998
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "10920001",
      "question": "What street is this?",
      "prediction_raw": "broadway",
      "prediction_used_for_eval": "broadway",
      "human_answers_used_for_eval": [
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "broadway",
        "new york street",
        "broadway",
        "broadway",
        "broadway",
        "broadway"
      ],
      "used_normalization": true,
      "matches_m_total": 9,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "11184008",
      "question": "Where is the mouse?",
      "prediction_raw": "top right",
      "prediction_used_for_eval": "top right",
      "human_answers_used_for_eval": [
        "on keyboard",
        "charging",
        "keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "on keyboard",
        "sitting on keyboard"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "11291003",
      "question": "Is this a formal affair?",
      "prediction_raw": "casual",
      "prediction_used_for_eval": "casual",
      "human_answers_used_for_eval": [
        "yes",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "no",
        "yes",
        "no"
      ],
      "used_normalization": true,
      "matches_m_total": 0,
      "accuracy_official_vqa": 0.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "11364000",
      "question": "Do the trees have leaf's on them?",
      "prediction_raw": "yes",
      "prediction_used_for_eval": "yes",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "yes"
      ],
      "used_normalization": false,
      "matches_m_total": 10,
      "accuracy_official_vqa": 1.0
    },
    {
      "preprocessor": "YoloV12SalientRoi+GlobalThumb",
      "sample_id": "12047000",
      "question": "Did the artist employ a nauseating amount of tacky photo manipulation?",
      "prediction_raw": "no",
      "prediction_used_for_eval": "no",
      "human_answers_used_for_eval": [
        "yes",
        "yes",
        "yes",
        "yes",
        "yes",
        "no",
        "yes",
        "yes",
        "no",
        "yes"
      ],
      "used_normalization": true,
      "matches_m_total": 2,
      "accuracy_official_vqa": 0.6
    }
  ]
}